# Causal Embeddings for Recommendation

ITE: 控制非随机的数据来估计因果效应

这篇文章介绍一种改进的矩阵分解方法，这是利用一个小样本的随机推荐结果，来创建用户和产品的表示。我们的方法中，用户和物品对之间的成对距离与传统方法相比对应的更一致。

### 推荐策略

对于一个推荐系统而言，输入是一个用户 $u_i$，输出是一个预测，这个预测是可能推荐的产品 $p_j$。推荐系统的外部行为可以被描述为：对于每个用户而言，应该展现出什么产品。用户 $u_i$ 接触这个推荐产品 $p_j$ 的概率：

$p_j\sim \pi_x(·|u_i)$ 

为了简化，**我们假设一件产品也不展示也是一种干预**。

### 策略收益 Policy rewards

我们定义 $r_{ij}$ 对于给用户 $u_i$ 推荐产品 $p_j$ 是一个真实的结果或者收益。我们假设 $r_{ij}$ 对于用户 $u_i$ 和产品 $p_j$ 是根据一个位置分布 $r$ 分布的：

$r_{ij}\sim r(·|u_i,p_j)$

我们定义 $y_{ij}$ 是被观察的收益，根据日志策略 $\pi_x$ :

$y_{ij}=r_{ij}\pi_x(p_j|u_i)$

关于策略 $\pi_x$ 的奖励值 $R^{\pi_x}$ 等价于通过使用相关的个性化产品曝光概率而获得的所有正在进入用户的奖励的总和：

$R^{\pi_x}=\sum_{ij} r_{ij}\pi_x(p_j|u_i)p(u_i)=\sum_{ij} y_{ij}p(u_i)=\sum_{ij} R_{ij}$

一个策略的 ITE 值是可以被定义为它的收益和控制策略 $\pi_c$ 下的收益之差：

$ITE_{ij}^{\pi_x} =R_{ij}^{\pi_x}-R_{ij}^{\pi_c}$

在这篇论文中，我们感兴趣的是找到 ITE 之和最高的策略 $\pi^*$:

$\pi^*=argmax_{\pi_x}\{\sum_{ij}ITE_{ij}^{\pi_x}\}$





































